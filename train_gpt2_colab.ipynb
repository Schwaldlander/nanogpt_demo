{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Schwaldlander/nanogpt_demo/blob/main/train_gpt2_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e44900bc",
      "metadata": {
        "id": "e44900bc"
      },
      "source": [
        "# Fine‑tune GPT‑2 from scratch on EDU FineWeb‑10B\n",
        "\n",
        "This Colab notebook refactors **`train_gpt2.py`** (from https://github.com/karpathy/build-nanogpt/blob/master/train_gpt2.py) into a step‑by‑step workflow. Releant explanation can be found in [https://www.youtube.com/watch?v=l8pRSuU81PU]:\n",
        "\n",
        "1. **Install** required libraries  \n",
        "2. **Define** GPT‑2 building blocks  \n",
        "3. **Prepare** an ultra‑lightweight streaming dataloader  \n",
        "4. **Configure** (optional) Distributed Data Parallel (DDP)  \n",
        "5. **Train**, **validate** on HellaSwag, and **sample** text  \n",
        "\n",
        "Feel free to tweak hyper‑parameters such as `max_steps` and `total_batch_size` to match your compute budget.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "96cd4ef4",
      "metadata": {
        "id": "96cd4ef4"
      },
      "outputs": [],
      "source": [
        "# @title Install dependencies\n",
        "!pip install -q tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"import fsspec; fsspec.filesystem('hf')\""
      ],
      "metadata": {
        "id": "FYZtwcurH40g"
      },
      "id": "FYZtwcurH40g",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \"datasets>=2.15\" \"huggingface_hub[fsspec]\" fsspec"
      ],
      "metadata": {
        "id": "iOqlIIEOTkaq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec9e9a57-4301-478d-9908-9c8ad375e428"
      },
      "id": "iOqlIIEOTkaq",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets>=2.15 in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (2025.3.0)\n",
            "Collecting fsspec\n",
            "  Using cached fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface_hub[fsspec] in /usr/local/lib/python3.11/dist-packages (0.33.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.15) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.15) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.15) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.15) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.15) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.15) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.15) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.15) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.15) (0.70.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets>=2.15) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.15) (6.0.2)\n",
            "\u001b[33mWARNING: huggingface-hub 0.33.2 does not provide the extra 'fsspec'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[fsspec]) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[fsspec]) (1.1.5)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.15) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.15) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.15) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.15) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.15) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.15) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.15) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.15) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.15) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.15) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.15) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.15) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.15) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.15) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.15) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.15) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli whoami"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02xYn4CuxF6i",
        "outputId": "c741bf2d-1a69-453f-d9f5-bc3c2dc9e63d"
      },
      "id": "02xYn4CuxF6i",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GeorgesMiradaHas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "4z4nvPg-Tjxw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed602d50-f2cb-4c53-a9db-c972f5e9e10a"
      },
      "id": "4z4nvPg-Tjxw",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `Sevres` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `Sevres`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c7e347e",
      "metadata": {
        "id": "0c7e347e"
      },
      "source": [
        "## Imports\n",
        "\n",
        "hellasawg library is a challenge dataset of filling sentences. Project Link[https://huggingface.co/datasets/Rowan/hellaswag]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f9f62598",
      "metadata": {
        "id": "f9f62598"
      },
      "outputs": [],
      "source": [
        "import os, math, time, inspect\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "# from hellaswag import render_example, iterate_examples\n",
        "import datasets\n",
        "import torch, itertools\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "15b0290a",
      "metadata": {
        "id": "15b0290a"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "        # regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
        "        qkv = self.c_attn(x)\n",
        "        # normally we don't define three separate linear layers, but project input to 3times dim\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6c0afe8",
      "metadata": {
        "id": "c6c0afe8"
      },
      "source": [
        "## Transformer building blocks\n",
        "\n",
        "Unlike ReLU, GELU returns non-zero values when $x<0$, given by\n",
        "\n",
        "$\\operatorname{GELU}(x) \\;\\approx\\; 0.5 \\, x \\,\\Bigl(1 + \\tanh\\!\\Bigl[\\sqrt{\\tfrac{2}{\\pi}}\\,\n",
        "  \\bigl(x + 0.044715\\,x^{3}\\bigr)\\Bigr]\\Bigr)$\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc   = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu   = nn.GELU(approximate='tanh')\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "# Residual connection to control activation variance growth in the stream in forward propogation,\n",
        "#and to preserve information and gradient flow, making very deep nets trainable\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp  = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "HyQB4IA9jgjm"
      },
      "id": "HyQB4IA9jgjm",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c3547693",
      "metadata": {
        "id": "c3547693"
      },
      "source": [
        "## GPT‑2 model wrapper\n",
        "\n",
        "n_embd is chosen following GPT2 paper. As for the size of T, Do not try to finetune GPT-2 on longer sequences unless you are prepared to alter its architecture.\n",
        "\n",
        "Note that LayerNorm is **after** Transformer modules to bring stabler gradients in very deep nets; allows cranking the learning-rate without warm-up gymnastics.\n",
        "\n",
        "# FlashAttention\n",
        " consists of kernel fusion operation. It is high performance implementation of the attention mechanism in transformers. It is fast, memory-efficient and numerically stable, by reducing GPU I/O ops.\n",
        "\n",
        " 1. Computes blocks of attention scores in SRAM GPU registers, instead of materializing large NxN attention matrix in High Bandwidth Memory (HBM)\n",
        "\n",
        " 2. Use tiling and fused kernel operations to reduce memory reads/writes\n",
        "\n",
        " 3. Applies softmax + dropout + matmal in one fused kernel, though inference paths usually compile without dropout for speed.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "94854ca1",
      "metadata": {
        "id": "94854ca1"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50257\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(\n",
        "            dict(\n",
        "                wte   = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "                wpe   = nn.Embedding(config.block_size, config.n_embd),\n",
        "                h     = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "                ln_f  = nn.LayerNorm(config.n_embd),\n",
        "            )\n",
        "        )\n",
        "        #use learnable position encoding\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        # weight sharing that enhances efficiency and proves effective\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std = 0.02\n",
        "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
        "                std *= (2 * self.config.n_layer) ** -0.5\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, \"Sequence length > block size\"\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        x = tok_emb + pos_emb\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        # Notice that the third layernorm before lm_head\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4de9d08b",
      "metadata": {
        "id": "4de9d08b"
      },
      "source": [
        "## Streaming dataloader & helpers\n",
        "\n",
        "Note that not every dataset token is processed by GPT training. The vocab size is larger than actual size to make itself divisble by 2-exponents. Throughput is increased despite vocal_size raised.\n",
        "\n",
        "Data Augementation(noise adding, shuffling) not added"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "class StreamingBatchLoader:\n",
        "    \"\"\"\n",
        "    Replaces the original GPTDataset in nanoGPT for streaming use.\n",
        "    Generates (x, y) token blocks from a streaming Hugging Face dataset.\n",
        "    \"\"\"\n",
        "    def __init__(self, repo_id, version_name, split=\"train\", block_size=1024, batch_size=8, seed=42):\n",
        "        self.block_size = block_size\n",
        "        self.batch_size = batch_size\n",
        "        self.buf = []\n",
        "\n",
        "        # Load tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token  # For safety\n",
        "\n",
        "        # Load streaming dataset\n",
        "        ds = load_dataset(\n",
        "            repo_id,\n",
        "            name=version_name,\n",
        "            split=\"train\",\n",
        "            streaming=True)\n",
        "        ds = ds.shuffle(buffer_size=10_000, seed=seed)\n",
        "        ds = ds.map(self._tokenize, remove_columns=[\"text\"])\n",
        "        self.iterator = iter(ds)\n",
        "\n",
        "    def _tokenize(self, example):\n",
        "        ids = self.tokenizer.encode(example[\"text\"]) + [self.tokenizer.eos_token_id]\n",
        "        return {\"ids\": ids}\n",
        "\n",
        "    def next(self):\n",
        "        \"\"\"\n",
        "        Returns one (x, y) batch of shape (B, T)\n",
        "        \"\"\"\n",
        "        required = self.batch_size * self.block_size + 1\n",
        "        while len(self.buf) < required:\n",
        "            self.buf.extend(next(self.iterator)[\"ids\"])\n",
        "\n",
        "        tokens = torch.tensor(self.buf[:required], dtype=torch.long)\n",
        "        del self.buf[:self.batch_size * self.block_size]\n",
        "\n",
        "        x = tokens[:-1].view(self.batch_size, self.block_size)\n",
        "        y = tokens[1:].view(self.batch_size, self.block_size)\n",
        "        return x, y\n",
        "\n",
        "train_loader = StreamingBatchLoader(repo_id=\"wikitext\", version_name=\"wikitext-103-v1\",split=\"train\", block_size=1024, batch_size=8)\n",
        "# x, y = train_loader.next()\n",
        "# print(x.shape, y.shape)"
      ],
      "metadata": {
        "id": "C7zMfuhexRfo"
      },
      "id": "C7zMfuhexRfo",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6954ab26",
      "metadata": {
        "id": "6954ab26"
      },
      "source": [
        "## Device / DDP setup & hyper‑parameters\n",
        "\n",
        "Distributed Data Parallel(DDP) saves GPU computation resources. Every GPU has its separate model and dataloader.  \n",
        "\n",
        "RANK = 0 means main process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "cac210a0",
      "metadata": {
        "id": "cac210a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b41ef6b-7947-40f9-8710-14bd8dcfd35f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda rank 0\n",
            "Gradient accumulation steps: 1\n"
          ]
        }
      ],
      "source": [
        "# Detect distributed run\n",
        "ddp = int(os.environ.get('RANK', -1)) != -1\n",
        "if ddp:\n",
        "    import torch.distributed as dist\n",
        "    from torch.distributed import init_process_group\n",
        "    from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "\n",
        "    init_process_group(backend='nccl')\n",
        "    ddp_rank = int(os.environ['RANK'])\n",
        "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
        "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
        "    device = f'cuda:{ddp_local_rank}'\n",
        "    torch.cuda.set_device(device)\n",
        "    master_process = ddp_rank == 0\n",
        "else:\n",
        "    ddp_rank = 0\n",
        "    ddp_world_size = 1\n",
        "    master_process = True\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "device_type = 'cuda' if device.startswith('cuda') else 'cpu'\n",
        "print('Using device:', device, 'rank', ddp_rank)\n",
        "\n",
        "# Hyper‑parameters (tweak to taste)\n",
        "#total_batch_size = 524288  # 2**19, approx 0.5M, in number of tokens for Fineweb edu\n",
        "total_batch_size = 8192 # for WikiText\n",
        "B = 8#64\n",
        "T = 1024\n",
        "assert total_batch_size % (B * T * ddp_world_size) == 0\n",
        "grad_accum_steps = total_batch_size // (B * T * ddp_world_size)\n",
        "print('Gradient accumulation steps:', grad_accum_steps)\n",
        "\n",
        "max_lr   = 6e-4\n",
        "min_lr   = max_lr * 0.1\n",
        "warmup_steps = 715\n",
        "max_steps  = 500  # shorter default for Colab; raise for full training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b254cd94",
      "metadata": {
        "id": "b254cd94"
      },
      "source": [
        "## Build model & dataloaders\n",
        "\n",
        "Before reaching warmup iterations, learning rate increases linearly. After the set steps, cosine decay is applied.\n",
        "\n",
        "In AdamW optimizer, decoupled weight decay is used as L2 regularization to penalize large weights and avoid overfitting. Compared to SGD, it reduces the ampitude of gradient flucations.\n",
        "\n",
        "Aside from the code implementations, it is also feasible to dynamically increase batch size."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT(GPTConfig())\n",
        "model.to(device)\n",
        "if ddp:\n",
        "    model = DDP(model, device_ids=[ddp_local_rank])\n",
        "raw_model = model.module if ddp else model\n",
        "\n",
        "def get_lr(it):\n",
        "    if it < warmup_steps:\n",
        "        return max_lr * (it + 1) / warmup_steps\n",
        "    if it > max_steps:\n",
        "        return min_lr\n",
        "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "    return min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    raw_model.parameters(),\n",
        "    lr=max_lr, betas=(0.9, 0.95), eps=1e-8, weight_decay=0.1, fused=('fused' in inspect.signature(torch.optim.AdamW).parameters and device_type=='cuda')\n",
        ")\n"
      ],
      "metadata": {
        "id": "tKORgNTIKs7M"
      },
      "id": "tKORgNTIKs7M",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "dca2735b",
      "metadata": {
        "id": "dca2735b"
      },
      "source": [
        "## Training loop\n",
        "Typically the tiktoken compress rate is 3:1\n",
        "\n",
        "When we wish to increase batch size but constainted to memory size, we can accumulate gradient on multiple mini batches, and defer backward propogation until all mini batches in one batch are processed.\n",
        "\n",
        "If model.require_backward_grad_sync=True, then DDP uses al-reduce operation to average gradients on all gpus, then send back to ensure all the gradients are synchornized.\n",
        "\n",
        "**why autocast to bfloat16:**\n",
        "\n",
        "16-bit numbers use half the memory of 32-bit. bfloat16 has the same exponent range as float32, but fewer mantissa bits, so it can represent very large/small numbers **without** underflow/overflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "0834a6b5",
      "metadata": {
        "id": "0834a6b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "outputId": "efeb4383-2f44-4df4-c3d8-24cb17cc82e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0000 | loss 10.9948 | lr 8.39e-07 | time 4452 ms\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 1.54 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.21 GiB is free. Process 70255 has 13.53 GiB memory in use. Of the allocated memory 12.98 GiB is allocated by PyTorch, and 429.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-16-2271045431.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mgrad_accum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mloss_accum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.54 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.21 GiB is free. Process 70255 has 13.53 GiB memory in use. Of the allocated memory 12.98 GiB is allocated by PyTorch, and 429.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "enc = tiktoken.get_encoding('gpt2')\n",
        "\n",
        "for step in range(max_steps):\n",
        "    t0 = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    loss_accum = 0.0\n",
        "\n",
        "    for micro_step in range(grad_accum_steps):\n",
        "        x, y = train_loader.next()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        if ddp:\n",
        "            model.require_backward_grad_sync = (micro_step == grad_accum_steps - 1)\n",
        "        # increase throughput with bfloat16\n",
        "        with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
        "            logits, loss = model(x, y)\n",
        "        (loss / grad_accum_steps).backward()\n",
        "        loss_accum += loss.detach()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    lr = get_lr(step)\n",
        "    for pg in optimizer.param_groups:\n",
        "        pg['lr'] = lr\n",
        "    optimizer.step() # updates parameters\n",
        "\n",
        "    dt = time.time() - t0\n",
        "    if master_process:\n",
        "        print(f'step {step:04d} | loss {loss_accum.item():.4f} | lr {lr:.2e} | time {dt*1000:.0f} ms')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f726c0f",
      "metadata": {
        "id": "3f726c0f"
      },
      "source": [
        "## Quick top‑k sampling check\n",
        "\n",
        "This displays the performance of code: how the LLM fills sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ff3b251",
      "metadata": {
        "id": "2ff3b251"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "prompt = \"Hello, I'm a French student,\"\n",
        "tokens = torch.tensor(enc.encode(prompt), dtype=torch.long, device=device).unsqueeze(0)\n",
        "max_len = 32\n",
        "for _ in range(max_len - tokens.size(1)):\n",
        "    with torch.no_grad(), torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
        "        logits, _ = model(tokens)\n",
        "    probs = torch.softmax(logits[:, -1], dim=-1)\n",
        "    idx = torch.multinomial(probs, num_samples=1)\n",
        "    tokens = torch.cat([tokens, idx], dim=-1)\n",
        "\n",
        "print(enc.decode(tokens[0].tolist()))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}