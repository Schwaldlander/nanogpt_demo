{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Schwaldlander/nanogpt_demo/blob/main/train_gpt2_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e44900bc",
      "metadata": {
        "id": "e44900bc"
      },
      "source": [
        "# Fine‑tune GPT‑2 from scratch on EDU FineWeb‑10B\n",
        "\n",
        "This Colab notebook refactors **`train_gpt2.py`** (from https://github.com/karpathy/build-nanogpt/blob/master/train_gpt2.py) into a step‑by‑step workflow. Releant explanation can be found in [https://www.youtube.com/watch?v=l8pRSuU81PU]:\n",
        "\n",
        "1. **Install** required libraries  \n",
        "2. **Define** GPT‑2 building blocks  \n",
        "3. **Prepare** an ultra‑lightweight streaming dataloader  \n",
        "4. **Configure** (optional) Distributed Data Parallel (DDP)  \n",
        "5. **Train**, **validate** on HellaSwag, and **sample** text  \n",
        "\n",
        "Feel free to tweak hyper‑parameters such as `max_steps` and `total_batch_size` to match your compute budget.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "96cd4ef4",
      "metadata": {
        "id": "96cd4ef4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "383c1aec-2ce2-41e7-9304-cee8bb2efb01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement hellaswag (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for hellaswag\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# @title Install dependencies\n",
        "!pip install -q tiktoken hellaswag"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c7e347e",
      "metadata": {
        "id": "0c7e347e"
      },
      "source": [
        "## Imports\n",
        "\n",
        "hellasawg library is a challenge dataset of filling sentences. Project Link[https://huggingface.co/datasets/Rowan/hellaswag]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f9f62598",
      "metadata": {
        "id": "f9f62598",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "dec03d7c-3fdb-43c3-eb12-fa4617b06e1c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'hellaswag'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-1759928324.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtiktoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhellaswag\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrender_example\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterate_examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hellaswag'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os, math, time, inspect\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "from hellaswag import render_example, iterate_examples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6c0afe8",
      "metadata": {
        "id": "c6c0afe8"
      },
      "source": [
        "## Transformer building blocks\n",
        "\n",
        "Unlike ReLU, GELU returns non-zero values when $x<0$, given by\n",
        "$\\operatorname{GELU}(x) \\;\\approx\\; 0.5 \\, x \\,\\Bigl(1 + \\tanh\\!\\Bigl[\\sqrt{\\tfrac{2}{\\pi}}\\,\n",
        "  \\bigl(x + 0.044715\\,x^{3}\\bigr)\\Bigr]\\Bigr)$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15b0290a",
      "metadata": {
        "id": "15b0290a"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "        # regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
        "        qkv = self.c_attn(x)\n",
        "        # normally we don't define three separate linear layers, but project input to 3times dim\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc   = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu   = nn.GELU(approximate='tanh')\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "# Residual connection to control activation variance growth in the stream in forward propogation\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp  = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3547693",
      "metadata": {
        "id": "c3547693"
      },
      "source": [
        "## GPT‑2 model wrapper\n",
        "\n",
        "n_embd is chosen following GPT2 paper.\n",
        "\n",
        "# FlashAttention\n",
        " kernel fusion operation. It is high performance implementation of the attention mechanism in transformers. It is fast, memory-efficient and numerically stable, by reducing GPU I/O ops.\n",
        "\n",
        " 1. Computes blocks of attention scores in GPU registers\n",
        "\n",
        " 2. Use tiling and fused kernel operations to reduce memory reads/writes\n",
        "\n",
        " 3. Applies softmax + dropout + matmal in one fused kernel\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94854ca1",
      "metadata": {
        "id": "94854ca1"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50257\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(\n",
        "            dict(\n",
        "                wte   = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "                wpe   = nn.Embedding(config.block_size, config.n_embd),\n",
        "                h     = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "                ln_f  = nn.LayerNorm(config.n_embd),\n",
        "            )\n",
        "        )\n",
        "        #use learnable position encoding\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        # weight sharing that enhances efficiency and proves effective\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std = 0.02\n",
        "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
        "                std *= (2 * self.config.n_layer) ** -0.5\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, \"Sequence length > block size\"\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        x = tok_emb + pos_emb\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        # Notice that the third layernorm before lm_head\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4de9d08b",
      "metadata": {
        "id": "4de9d08b"
      },
      "source": [
        "## Streaming dataloader & helpers\n",
        "\n",
        "Note that not every dataset token is processed by GPT training.\n",
        "\n",
        "Data Augementation(noise adding, shuffling) not added"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8787757b",
      "metadata": {
        "id": "8787757b"
      },
      "outputs": [],
      "source": [
        "def load_tokens(filename):\n",
        "    npt = np.load(filename).astype(np.int32)\n",
        "    return torch.tensor(npt, dtype=torch.long)\n",
        "\n",
        "\n",
        "class DataLoaderLite:\n",
        "    \"\"\"Ultra‑lightweight streaming dataloader.\"\"\"\n",
        "    def __init__(self, B, T, process_rank, num_processes, split):\n",
        "        self.B = B\n",
        "        self.T = T\n",
        "        self.process_rank = process_rank\n",
        "        self.num_processes = num_processes\n",
        "        assert split in {'train', 'val'}\n",
        "\n",
        "        data_root = \"edu_fineweb10B\"\n",
        "        shards = sorted([os.path.join(data_root, s) for s in os.listdir(data_root) if split in s])\n",
        "        assert shards, f\"No shards found for split {split}\"\n",
        "        self.shards = shards\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_shard = 0\n",
        "        self.tokens = load_tokens(self.shards[self.current_shard])\n",
        "        self.current_position = self.B * self.T * self.process_rank\n",
        "\n",
        "    def next_batch(self):\n",
        "        B, T = self.B, self.T\n",
        "        buf = self.tokens[self.current_position : self.current_position + B*T + 1]\n",
        "        x = buf[:-1].view(B, T)\n",
        "        y = buf[1:].view(B, T)\n",
        "        self.current_position += B * T * self.num_processes\n",
        "        # prevent overflow\n",
        "        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n",
        "            self.current_shard = (self.current_shard + 1) % len(self.shards)\n",
        "            self.tokens = load_tokens(self.shards[self.current_shard])\n",
        "            self.current_position = B * T * self.process_rank\n",
        "        return x, y\n",
        "\n",
        "\n",
        "def get_most_likely_row(tokens, mask, logits):\n",
        "    shift_logits = logits[..., :-1, :].contiguous()\n",
        "    shift_tokens = tokens[..., 1:].contiguous()\n",
        "    flat_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
        "    flat_tokens = shift_tokens.view(-1)\n",
        "    shift_losses = F.cross_entropy(flat_logits, flat_tokens, reduction='none').view(tokens.size(0), -1)\n",
        "    shift_mask = mask[..., 1:].contiguous()\n",
        "    avg_loss = (shift_losses * shift_mask).sum(dim=1) / shift_mask.sum(dim=1)\n",
        "    return avg_loss.argmin().item()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6954ab26",
      "metadata": {
        "id": "6954ab26"
      },
      "source": [
        "## Device / DDP setup & hyper‑parameters\n",
        "\n",
        "Distributed Data Parallel(DDP) saves GPU computation resources. Every GPU has its separate model and dataloader.  \n",
        "\n",
        "RANK = 0 means main process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cac210a0",
      "metadata": {
        "id": "cac210a0"
      },
      "outputs": [],
      "source": [
        "# Detect distributed run\n",
        "ddp = int(os.environ.get('RANK', -1)) != -1\n",
        "if ddp:\n",
        "    import torch.distributed as dist\n",
        "    from torch.distributed import init_process_group\n",
        "    from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "\n",
        "    init_process_group(backend='nccl')\n",
        "    ddp_rank = int(os.environ['RANK'])\n",
        "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
        "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
        "    device = f'cuda:{ddp_local_rank}'\n",
        "    torch.cuda.set_device(device)\n",
        "    master_process = ddp_rank == 0\n",
        "else:\n",
        "    ddp_rank = 0\n",
        "    ddp_world_size = 1\n",
        "    master_process = True\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "device_type = 'cuda' if device.startswith('cuda') else 'cpu'\n",
        "print('Using device:', device)\n",
        "\n",
        "# Hyper‑parameters (tweak to taste)\n",
        "total_batch_size = 524288  # tokens\n",
        "B = 64\n",
        "T = 1024\n",
        "assert total_batch_size % (B * T * ddp_world_size) == 0\n",
        "grad_accum_steps = total_batch_size // (B * T * ddp_world_size)\n",
        "print('Gradient accumulation steps:', grad_accum_steps)\n",
        "\n",
        "max_lr   = 6e-4\n",
        "min_lr   = max_lr * 0.1\n",
        "warmup_steps = 715\n",
        "max_steps  = 500  # shorter default for Colab; raise for full training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b254cd94",
      "metadata": {
        "id": "b254cd94"
      },
      "source": [
        "## Build model & dataloaders\n",
        "\n",
        "Before reaching warmup iterations, learning rate increases linearly. After the set steps, cosine decay is applied.\n",
        "\n",
        "In optimizer, weight decay is used as L2 regularization to penalize large weights and avoid overfitting.\n",
        "\n",
        "Aside from the code implementations, it is also feasible to dynamically increase batch size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "283598aa",
      "metadata": {
        "id": "283598aa"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoaderLite(B, T, ddp_rank, ddp_world_size, split='train')\n",
        "val_loader   = DataLoaderLite(B, T, ddp_rank, ddp_world_size, split='val')\n",
        "\n",
        "model = GPT(GPTConfig())\n",
        "model.to(device)\n",
        "if ddp:\n",
        "    model = DDP(model, device_ids=[ddp_local_rank])\n",
        "raw_model = model.module if ddp else model\n",
        "\n",
        "def get_lr(it):\n",
        "    if it < warmup_steps:\n",
        "        return max_lr * (it + 1) / warmup_steps\n",
        "    if it > max_steps:\n",
        "        return min_lr\n",
        "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "    return min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    raw_model.parameters(),\n",
        "    lr=max_lr, betas=(0.9, 0.95), eps=1e-8, weight_decay=0.1, fused=('fused' in inspect.signature(torch.optim.AdamW).parameters and device_type=='cuda')\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dca2735b",
      "metadata": {
        "id": "dca2735b"
      },
      "source": [
        "## Training loop\n",
        "Typically the tiktoken compress rate is 3:1\n",
        "\n",
        "When we wish to increase batch size but constainted to memory size, we can accumulate gradient on multiple mini batches, and defer backward propogation until all mini batches in one batch are processed.\n",
        "\n",
        "If model.require_backward_grad_sync=True, then DDP uses al-reduce operation to average gradients on all gpus, then send back to ensure all the gradients are synchornized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0834a6b5",
      "metadata": {
        "id": "0834a6b5"
      },
      "outputs": [],
      "source": [
        "enc = tiktoken.get_encoding('gpt2')\n",
        "\n",
        "for step in range(max_steps):\n",
        "    t0 = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    loss_accum = 0.0\n",
        "\n",
        "    for micro_step in range(grad_accum_steps):\n",
        "        x, y = train_loader.next_batch()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        if ddp:\n",
        "            model.require_backward_grad_sync = (micro_step == grad_accum_steps - 1)\n",
        "        with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
        "            logits, loss = model(x, y)\n",
        "        (loss / grad_accum_steps).backward()\n",
        "        loss_accum += loss.detach()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    lr = get_lr(step)\n",
        "    for pg in optimizer.param_groups:\n",
        "        pg['lr'] = lr\n",
        "    optimizer.step() # updates parameters\n",
        "\n",
        "    dt = time.time() - t0\n",
        "    if master_process:\n",
        "        print(f'step {step:04d} | loss {loss_accum.item():.4f} | lr {lr:.2e} | time {dt*1000:.0f} ms')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f726c0f",
      "metadata": {
        "id": "3f726c0f"
      },
      "source": [
        "## Quick top‑k sampling check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ff3b251",
      "metadata": {
        "id": "2ff3b251"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "prompt = \"Hello, I'm a language model,\"\n",
        "tokens = torch.tensor(enc.encode(prompt), dtype=torch.long, device=device).unsqueeze(0)\n",
        "max_len = 32\n",
        "for _ in range(max_len - tokens.size(1)):\n",
        "    with torch.no_grad(), torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
        "        logits, _ = model(tokens)\n",
        "    probs = torch.softmax(logits[:, -1], dim=-1)\n",
        "    idx = torch.multinomial(probs, num_samples=1)\n",
        "    tokens = torch.cat([tokens, idx], dim=-1)\n",
        "\n",
        "print(enc.decode(tokens[0].tolist()))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}